{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Support Vector Machines for malware classification\n",
    "\n",
    "In this notebook we will experiment with SVM (Support Vector Machine) classifiers.\n",
    "\n",
    "Let's start by importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.externals import joblib\n",
    "from preprocessing import pp_action\n",
    "from helpers import loader_tfidf\n",
    "from utilities import constants\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as ply\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('config.json', 'r'))\n",
    "uuids_family = json.load(open(os.path.join(constants.dir_d, constants.json_labels), 'r'))\n",
    "words = json.load(open(os.path.join(constants.dir_d, constants.json_words), 'r'))\n",
    "ply.init_notebook_mode(connected=True)\n",
    "load_batch_size = 1100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "Select a subset of the original dataset. Then the selected subset will be split into a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data = pp_action.pre_process(config)\n",
    "pp_action.split_show_data(samples_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uuids = samples_data.index[samples_data['selected'] == 1].tolist()\n",
    "x_train = samples_data.index[samples_data['train'] == 1].tolist()\n",
    "x_dev = samples_data.index[samples_data['dev'] == 1].tolist()\n",
    "x_test = samples_data.index[samples_data['test'] == 1].tolist()\n",
    "y_train = samples_data.fam_num[samples_data['train'] == 1].tolist()\n",
    "y_dev = samples_data.fam_num[samples_data['dev'] == 1].tolist()\n",
    "y_test = samples_data.fam_num[samples_data['test'] == 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "We would also like this approach to be scalable to the entire balanced dataset so we will load sparse representations of the data vectors.\n",
    "\n",
    "To achieve this we will use the Principal Component Analysis in order to operate on the sparse vectros. Let's define two helper functions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_pca(config, i_pca, samples, load_batch_size):\n",
    "    t = 0\n",
    "    \n",
    "    while t < len(samples):\n",
    "        data = loader_tfidf.load_tfidf(config, samples[t : t + load_batch_size], dense=True, ordered=False)\n",
    "        t += load_batch_size\n",
    "\n",
    "        i_pca.partial_fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_data(config, i_pca, samples, load_batch_size):\n",
    "    new_data = [] \n",
    "    t = 0\n",
    "    \n",
    "    while t < len(samples):\n",
    "        data = loader_tfidf.load_tfidf(config, samples[t : t + load_batch_size], dense=True, ordered=True)\n",
    "        t += load_batch_size\n",
    "\n",
    "        new_data.append(i_pca.transform(data))\n",
    "        \n",
    "    return np.concatenate(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_pca = IncrementalPCA(n_components=1024, batch_size=load_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the PCA algorithm incrementally only on the trainining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pca(config, i_pca, random.sample(x_train, len(x_train)), load_batch_size)\n",
    "joblib.dump(i_pca, 'temp_pca_1000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# or directly load the trained PCA model if available\n",
    "i_pca = joblib.load('temp_pca_1000.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i_pca.explained_variance_ratio_.sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will use the trained algorithm to (incrementally) transform all the data vectors. This will allow us to transform larger dataset than what would fit in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transform_data(config, i_pca, x_train, load_batch_size)\n",
    "X_dev = transform_data(config, i_pca, x_dev, load_batch_size)\n",
    "X_test = transform_data(config, i_pca, x_test, load_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels acquisition\n",
    "\n",
    "Let's store the true labels somewhere we can find them when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(set(y_train))\n",
    "n_classes = len(classes)\n",
    "\n",
    "classes_dict = dict(zip(classes, range(n_classes)))\n",
    "Y_train = np.array([classes_dict[i] for i in y_train])\n",
    "Y_dev = np.array([classes_dict[i] for i in y_dev])\n",
    "Y_test = np.array([classes_dict[i] for i in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_dev shape: \" + str(X_dev.shape))\n",
    "print (\"Y_dev shape: \" + str(Y_dev.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Once the dataset is ready we use the Scikit Learn library implementation of the SVM Classifier to classify our data points.\n",
    "\n",
    "Let's try different approaches. First we try a RBF (Radial Basis Function) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_r = SVC(kernel='rbf', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_r.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels_r = svc_r.predict(X_train)\n",
    "dev_labels_r = svc_r.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_score_r = f1_score(Y_train, train_labels_r, average='micro')\n",
    "dev_score_r = f1_score(Y_dev, dev_labels_r, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score on train set: {}'.format(train_score_r))\n",
    "print('F1 score on dev set: {}'.format(dev_score_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of performance is not really waht we are looking for. Let's try to see if something changes playing with the classification parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_2d_range = [1e-2, 1, 1e2]\n",
    "gamma_2d_range = [1e-1, 1, 1e1]\n",
    "classifiers = []\n",
    "for C in C_2d_range:\n",
    "    for gamma in gamma_2d_range:\n",
    "        clf = SVC(kernel='rbf', random_state=42, C=C, gamma=gamma)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        classifiers.append((C, gamma, clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classifiers:\n",
    "    c_c, c_g, c_s  = c\n",
    "    train_labels = c_s.predict(X_train)\n",
    "    dev_labels = c_s.predict(X_dev)\n",
    "    train_score = f1_score(Y_train, train_labels, average='micro')\n",
    "    dev_score = f1_score(Y_dev, dev_labels, average='micro')\n",
    "    print('SVM with C = {}, gamma = {}'.format(c_c, c_g))\n",
    "    print('F1 score on train set: {}'.format(train_score))\n",
    "    print('F1 score on dev set: {}'.format(dev_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance on the dev set with a RBF kernel does not look very promising. Let's try with a linear kernel instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_l = SVC(kernel='linear', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_l.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels_l = svc_l.predict(X_train)\n",
    "dev_labels_l = svc_l.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_score_l = f1_score(Y_train, train_labels_l, average='micro')\n",
    "dev_score_l = f1_score(Y_dev, dev_labels_l, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score on train set: {}'.format(train_score_l))\n",
    "print('F1 score on dev set: {}'.format(dev_score_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely a more promising result. Let's see if we can make it better by modifying the C parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_range = [1e-2, 1, 1e2]\n",
    "classifiers = []\n",
    "for C in C_range:\n",
    "    clf = SVC(kernel='linear', random_state=42, C=C)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    classifiers.append((C, clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classifiers:\n",
    "    c_c, c_s  = c\n",
    "    train_labels = c_s.predict(X_train)\n",
    "    dev_labels = c_s.predict(X_dev)\n",
    "    train_score = f1_score(Y_train, train_labels, average='micro')\n",
    "    dev_score = f1_score(Y_dev, dev_labels, average='micro')\n",
    "    print('SVM with C = {}'.format(c_c))\n",
    "    print('F1 score on train set: {}'.format(train_score))\n",
    "    print('F1 score on dev set: {}'.format(dev_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like C = 1 is the best value for C. Let's see what is the score on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_l = svc_l.predict(X_test)\n",
    "test_score_l = f1_score(Y_test, test_labels_l, average='micro')\n",
    "print('F1 score on train set: {}'.format(test_score_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
