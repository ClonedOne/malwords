{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using features extracted from Random Forest Classifiers\n",
    "\n",
    "In this notebook we are going to examine the possibility of selecting features for clustering based on the results of Random Forest Classifiers.\n",
    "\n",
    "Given the relatively good performances of RFCs on a supervised classification task, we want to explore the features selected by such classifiers in the context of unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import vis_data, vis_cluster\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from collections import defaultdict, Counter\n",
    "from dimensionality_reduction import dr_pca\n",
    "from classification import cla_rand_forest\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.externals import joblib\n",
    "from preprocessing import pp_action\n",
    "from clustering import clu_hdbscan\n",
    "from helpers import loader_tfidf\n",
    "from utilities import evaluation\n",
    "from utilities import constants\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as ply\n",
    "from sklearn.svm import SVC\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('config.json', 'r'))\n",
    "uuids_family = json.load(open(os.path.join(constants.dir_d, constants.json_labels), 'r'))\n",
    "words = json.load(open(os.path.join(constants.dir_d, constants.json_words), 'r'))\n",
    "inv_words = {num : word for word, num in words.items()}\n",
    "ply.init_notebook_mode(connected=True)\n",
    "max_feats = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "Select a subset of the original dataset. Then the selected subset will be split into a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples_data = pp_action.pre_process(config)\n",
    "pp_action.split_show_data(samples_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids = samples_data.index[samples_data['selected'] == 1].tolist()\n",
    "labels_num = samples_data.fam_num[samples_data['selected'] == 1].tolist()\n",
    "x_train = samples_data.index[samples_data['train'] == 1].tolist()\n",
    "x_dev = samples_data.index[samples_data['dev'] == 1].tolist()\n",
    "y_train = samples_data.fam_num[samples_data['train'] == 1].tolist()\n",
    "y_dev = samples_data.fam_num[samples_data['dev'] == 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading\n",
    "\n",
    "let's load the data in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = loader_tfidf.load_tfidf(config, x_train, dense=True, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev = loader_tfidf.load_tfidf(config, x_dev, dense=True, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader_tfidf.load_tfidf(config, uuids, dense=True, ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features\n",
    "\n",
    "This function isolates the `max_feats` most important features as identified by the list of feature weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_feats(feats_weights):\n",
    "    print(np.count_nonzero(feats_weights))\n",
    "    importance = defaultdict(list)\n",
    "    selected_feats = []\n",
    "    n_feats = 0\n",
    "    i = 0\n",
    "    \n",
    "    for imp in feats_weights:\n",
    "        importance[imp].append(i)\n",
    "        i += 1\n",
    "      \n",
    "    for imp in sorted(list(importance.keys()), reverse=True):\n",
    "        imp_feats = importance[imp]\n",
    "        to_add = len(imp_feats)\n",
    "\n",
    "        if n_feats + to_add > max_feats:\n",
    "            to_add = max_feats - n_feats\n",
    "\n",
    "        selected_feats += (sorted(imp_feats)[:to_add])\n",
    "        n_feats += to_add\n",
    "\n",
    "        if n_feats == max_feats:\n",
    "            break\n",
    "    \n",
    "    print(len(selected_feats))\n",
    "    return selected_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classification\n",
    "\n",
    "Here we define a helper function that evaluates the performance of a SVM classifier on the specified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_svm(t, y_t, d, y_d):\n",
    "    svm = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "    svm.fit(t, y_t)\n",
    "\n",
    "    classification_labels = svm.predict(d)\n",
    "    \n",
    "    print(f1_score(y_d, classification_labels, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Let's now define a function to use in order to evaluate the performance of hdbscan clustering on the specified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_hdbscan(d):\n",
    "    m = 'cosine'\n",
    "    distance = pairwise_distances(d, metric=m)\n",
    "    \n",
    "    hdbs = hdbscan.HDBSCAN(min_cluster_size=40,\n",
    "                           min_samples=None,\n",
    "                           metric='precomputed',\n",
    "                           core_dist_n_jobs=config['core_num'])\n",
    "    hdbs.fit(distance)\n",
    "    clustering_labels = hdbs.labels_\n",
    "    \n",
    "    evaluation.evaluate_clustering(labels_num, clustering_labels, data=d, metric=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial situation\n",
    "\n",
    "Let's see the performance of our algorithms with the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_svm(train, y_train, dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_hdbscan(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification\n",
    "\n",
    "Let's use random forests to classify the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classification_labels, randf = cla_rand_forest.classify(config, train, dev, x_dev, y_train, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feats_r = get_important_feats(randf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO method\n",
    "\n",
    "As an alternative to using random forests we may try the LASSO method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = Lasso(random_state=42, max_iter=2000, selection='random', positive=True, tol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feats_l = get_important_feats(las.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New dataset\n",
    "\n",
    "Now we can build a new dataset with reduced vectors using the features we just identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feats = selected_feats_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel = np.take(data, selected_feats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sel = np.take(train, selected_feats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sel = np.take(dev, selected_feats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_sel.shape)\n",
    "print(train_sel.shape)\n",
    "print(dev_sel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_svm(train_sel, y_train, dev_sel, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_hdbscan(data_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feats = selected_feats_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel = np.take(data, sorted(selected_feats), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sel = np.take(train, sorted(selected_feats), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sel = np.take(dev, sorted(selected_feats), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_sel.shape)\n",
    "print(train_sel.shape)\n",
    "print(dev_sel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_svm(train_sel, y_train, dev_sel, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_hdbscan(data_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
