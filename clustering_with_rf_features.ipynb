{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using features extracted from Random Forest Classifiers\n",
    "\n",
    "In this notebook we are going to examine the possibility of selecting features for clustering based on the results of Random Forest Classifiers.\n",
    "\n",
    "Given the relatively good performances of RFCs on a supervised classification task, we want to explore the features selected by such classifiers in the context of unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from visualization import vis_data, vis_cluster\n",
    "from collections import defaultdict, Counter\n",
    "from dimensionality_reduction import dr_pca\n",
    "from classification import cla_rand_forest\n",
    "from sklearn.externals import joblib\n",
    "from preprocessing import pp_action\n",
    "from clustering import clu_hdbscan\n",
    "from helpers import loader_tfidf\n",
    "from utilities import evaluation\n",
    "from utilities import constants\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as ply\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('config.json', 'r'))\n",
    "uuids_family = json.load(open(os.path.join(constants.dir_d, constants.json_labels), 'r'))\n",
    "words = json.load(open(os.path.join(constants.dir_d, constants.json_words), 'r'))\n",
    "ply.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "Select a subset of the original dataset. Then the selected subset will be split into a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples_data = pp_action.pre_process(config)\n",
    "pp_action.split_show_data(samples_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uuids = samples_data.index[samples_data['selected'] == 1].tolist()\n",
    "labels_num = samples_data.fam_num[samples_data['selected'] == 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification\n",
    "\n",
    "Let's use random forests to classify the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = samples_data.index[samples_data['train'] == 1].tolist()\n",
    "x_test = samples_data.index[samples_data['test'] == 1].tolist()\n",
    "y_train = samples_data.fam_num[samples_data['train'] == 1].tolist()\n",
    "y_test = samples_data.fam_num[samples_data['test'] == 1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = loader_tfidf.load_tfidf(config, x_train, dense=False, ordered=True)\n",
    "test = loader_tfidf.load_tfidf(config, x_test, dense=False, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classification_labels, randf = cla_rand_forest.classify(config, train, test, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = defaultdict(list)\n",
    "i = 0\n",
    "for imp in randf.feature_importances_:\n",
    "    importance[imp].append(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.count_nonzero(np.array(randf.feature_importances_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_feats = 512\n",
    "n_feats = 0\n",
    "selected_feats = []\n",
    "for imp in sorted(list(importance.keys()), reverse=True):\n",
    "    imp_feats = importance[imp]\n",
    "    to_add = len(imp_feats)\n",
    "    \n",
    "    if n_feats + to_add > max_feats:\n",
    "        to_add = max_feats - n_feats\n",
    "        \n",
    "    selected_feats += (sorted(imp_feats)[:to_add])\n",
    "    n_feats += to_add\n",
    "    \n",
    "    if n_feats == max_feats:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(selected_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprint(selected_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Let's now see what happens when we use those specific features for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = loader_tfidf.load_tfidf(\n",
    "    config, \n",
    "    uuids, \n",
    "    dense=True, \n",
    "    ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sel = np.take(data, sorted(selected_feats), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 'euclidean'\n",
    "\n",
    "hdbs = hdbscan.HDBSCAN(min_cluster_size=40,\n",
    "                       min_samples=None,\n",
    "                       metric=m,\n",
    "                       core_dist_n_jobs=config['core_num'])\n",
    "hdbs.fit(data_sel)\n",
    "computed_labels = hdbs.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluation.evaluate_clustering(labels_num, clustering_labels, data=data_sel, metric=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
