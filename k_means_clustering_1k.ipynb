{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means clustering result analysis\n",
    "\n",
    "We will start our exploration of the dataset with one of the most classical clustering algorithms: K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from clustering import clu_kmeans, clu_kmeans_minibatch\n",
    "from visualization import vis_data, vis_cluster\n",
    "from collections import defaultdict, Counter\n",
    "from keywords import kw_keyword_tfidf\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.externals import joblib\n",
    "from preprocessing import pp_action\n",
    "from helpers import loader_tfidf\n",
    "from utilities import constants\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as ply\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = json.load(open('config.json', 'r'))\n",
    "uuids_family = json.load(open(os.path.join(constants.dir_d, constants.json_labels), 'r'))\n",
    "words = json.load(open(os.path.join(constants.dir_d, constants.json_words), 'r'))\n",
    "ply.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "Select a subset of the original dataset. Then the selected subset will be split into a training and a testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples_data = pp_action.pre_process(config)\n",
    "pp_action.split_show_data(samples_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uuids = samples_data.index[samples_data['selected'] == 1].tolist()\n",
    "labels_num = samples_data.fam_num[samples_data['selected'] == 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Now that we have our data subset we can start with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering, clu_model = clu_kmeans_minibatch.cluster(config, 10, uuids, labels_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis\n",
    "\n",
    "To better understand the result of the clustering algorithm we would like to see the features characterizing the computed clusters. We can therefore aggregate the vectors composing each cluster in a single cumulative vector and retrieve the features with the highest weight in the cluster-vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kw_keyword_tfidf.extract_keywords(config, 'data/d_clusterings/clustering_kmeans_euclidean_minibatch_1209.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/d_keywords/clustering_kmeans_euclidean_minibatch_1209_keywords_tfidf', 'r') as kws:\n",
    "    print(kws.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also inspect the composition (based on our AV labels) of each cluster discovered by K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust_compositions = {i: Counter() for i in sorted(set(clustering.flatten()))}\n",
    "\n",
    "for i in range(len(uuids)):\n",
    "    clust_compositions[clustering[i]][uuids_family[uuids[i]]] += 1\n",
    "\n",
    "for clu in sorted(clust_compositions.keys()):\n",
    "    print('Cluster {}:'.format(clu))\n",
    "    print(clust_compositions[clu].most_common())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Visualization\n",
    "\n",
    "We can also generate a visual output from our clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the classification provided by the AV data with the result of our clustering, plotted over the same dimensionality reduced data points.\n",
    "\n",
    "Here, the color of the points will reflect the cluster in which they are assigned by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis_data.plot_data('data/d_matrices/tsne_2_1209.txt', clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the same comparison process with a 3-dimensional representation of the dataset. Since in this case tSNE generated a representation quite difficult to explore visually, we will use PCA to reduce the dimensions of our vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis_data.plot_data('data/d_matrices/pca_3_1209.txt', clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## With PCA reduced vectors\n",
    "\n",
    "It is also interesting to see hwo the clustering results change if we use the PCA reduced vectors instead of the full word wights vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you had already computed PCA, load it from the disk instead\n",
    "dr_model = joblib.load(os.path.join(constants.dir_d, constants.dir_mod, 'pca_128_1209.pkl')) \n",
    "reduced = np.loadtxt('data/d_matrices/pca_128_1209.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering, clu_model = clu_kmeans.cluster(config, 'data/d_matrices/pca_128_1209.txt', 10, uuids, labels_num, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverted_clustering = defaultdict(list)\n",
    "for i in range(len(uuids)):\n",
    "    inverted_clustering[clustering[i]].append(uuids[i])\n",
    "\n",
    "reduced_df = pd.DataFrame(reduced, index=uuids)\n",
    "centroids = {label : np.zeros(len(reduced[0])) for label in sorted(set(clustering))}\n",
    "\n",
    "i = 0\n",
    "for index, vector in reduced_df.iterrows():\n",
    "    centroids[clustering[i]] += vector.values\n",
    "    i += 1\n",
    "\n",
    "centroid_matrix = []\n",
    "for centroid in sorted(centroids.keys()):\n",
    "    centroids[centroid] /= len(inverted_clustering[centroid])\n",
    "    centroid_matrix.append(centroids[centroid])\n",
    "    \n",
    "centroid_matrix = np.array(centroid_matrix)\n",
    "centroids_orig_fts = np.dot(centroid_matrix, dr_model.components_)\n",
    "centroids_orig_fts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = dict(zip(range(len(words)), sorted(words.keys())))\n",
    "\n",
    "i = -1\n",
    "for centroid in centroids_orig_fts:\n",
    "    cent_series = pd.Series(np.abs(centroid), index=sorted(words.values()))\n",
    "    \n",
    "    print('Centroid {}:'.format(i))\n",
    "    print(cent_series.nlargest(10))\n",
    "    print()\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust_compositions = {i: Counter() for i in sorted(set(clustering.flatten()))}\n",
    "\n",
    "for i in range(len(uuids)):\n",
    "    clust_compositions[clustering[i]][uuids_family[uuids[i]]] += 1\n",
    "\n",
    "for clu in sorted(clust_compositions.keys()):\n",
    "    print('Cluster {}:'.format(clu))\n",
    "    print(clust_compositions[clu].most_common())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis_data.plot_data('data/d_matrices/tsne_2_1209.txt', clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis_data.plot_data('data/d_matrices/pca_3_1209.txt', clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
