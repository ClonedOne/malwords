{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN clustering result analysis\n",
    "\n",
    "Let's start by loading up some libraries and static data that may be useful in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from utilities import constants\n",
    "import plotly.offline as ply\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('config.json', 'r'))\n",
    "uuids_family = json.load(open(os.path.join(constants.dir_d, constants.json_labels), 'r'))\n",
    "words = json.load(open(os.path.join(constants.dir_d, constants.json_words), 'r'))\n",
    "ply.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "Select a subset of the original dataset. Then the selected subset will be split into a training and a testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import pp_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data = pp_action.pre_process(config)\n",
    "pp_action.split_show_data(samples_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Currently each data vector has approximately 300.000 components. High dimensionality feature vectors usually create problems during the clustering phase.\n",
    "\n",
    "Therefore, before going ahead to clustering the data, we proceed to reduce the dimensionality of the dataset.\n",
    "\n",
    "In this case we will use Principal Components Analysis to transfor our feature vectors in a new, dimensionally smaller, dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dimensionality_reduction import dr_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids = samples_data.index[samples_data['selected'] == 1].tolist()\n",
    "reduced, dr_model = dr_pca.reduce(config, uuids, 100)\n",
    "\n",
    "# If you had already computed PCA, load it from the disk instead\n",
    "# dr_model = joblib.load(os.path.join(constants.dir_d, constants.dir_mod, 'pca_X_X.pkl)) \n",
    "# reduced = np.loadtxt('matrix_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Once the data dimensionality has been reduced we can proceed with clustering. \n",
    "\n",
    "Here we will use HDBSCAN a hierarchical density-based clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from clustering import clu_hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_num = samples_data.fam_num[samples_data['selected'] == 1].tolist()\n",
    "clustering, clu_model = clu_hdbscan.cluster(config, 'c', uuids, labels_num, sparse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis\n",
    "\n",
    "To better understand the result of the clustering algorithm we would like to see the features characterizing the computed clusters. \n",
    "\n",
    "Since the dataset dimensionality was reduced with PCA before clustering we would need to reverse this step to understand the characteristics of the obtained clusters.\n",
    "\n",
    "To achieve this we will compute the centroids as the average of the data for each cluster and then multiply it by the transposed components matrix.\n",
    "\n",
    "We will start by creating an inverted index of the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_clustering = defaultdict(list)\n",
    "for i in range(len(uuids)):\n",
    "    inverted_clustering[clustering[i]].append(uuids[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pandas we can construct a dataframe representing our reduced data matrix with dimensions $ ( n\\_samples \\times n\\_pca\\_components) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_df = pd.DataFrame(reduced, index=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the centroids we will just average the values of the PCA-reduced features of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids = {label : np.zeros(len(reduced[0])) for label in sorted(set(clustering))}\n",
    "\n",
    "i = 0\n",
    "for index, vector in reduced_df.iterrows():\n",
    "    centroids[clustering[i]] += vector.values\n",
    "    i += 1\n",
    "\n",
    "centroid_matrix = []\n",
    "for centroid in sorted(centroids.keys()):\n",
    "    centroids[centroid] /= len(inverted_clustering[centroid])\n",
    "    centroid_matrix.append(centroids[centroid])\n",
    "    \n",
    "centroid_matrix = np.array(centroid_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the centroid matrix in the PCA space, we can bring it back to its original dimensions by multiplying it with the PCA components matrix.\n",
    "\n",
    "This will result in a $ ( n\\_centroids \\times n\\_original\\_features ) $ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_orig_fts = np.dot(centroid_matrix, dr_model.components_)\n",
    "centroids_orig_fts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once in the original dimension space we can identify the ten most influencial words for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dict(zip(range(len(words)), sorted(words.keys())))\n",
    "\n",
    "i = -1\n",
    "for centroid in centroids_orig_fts:\n",
    "    cent_series = pd.Series(np.abs(centroid), index=sorted(words.values()))\n",
    "    \n",
    "    print('Centroid {}:'.format(i))\n",
    "    print(cent_series.nlargest(10))\n",
    "    print()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may also be interesting to see which of the initial malware families compose each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_compositions = {i: Counter() for i in sorted(set(clustering.flatten()))}\n",
    "\n",
    "for i in range(len(uuids)):\n",
    "    clust_compositions[clustering[i]][uuids_family[uuids[i]]] += 1\n",
    "\n",
    "for clu in sorted(clust_compositions.keys()):\n",
    "    print('Cluster {}:'.format(clu))\n",
    "    print(clust_compositions[clu].most_common())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Visualization\n",
    "\n",
    "We can also generate a visual output from our clustering. \n",
    "\n",
    "Let's start by visualizing the original dataset. Since the ~300000 original features would not allow us to plot the data, we will use a 2-dimensional tSNE reduced version of our feature vectors.\n",
    "\n",
    "The color of each data point will be defined by the AV label extracted form VirusTotal using AVClass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from visualization import vis_data, vis_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = samples_data.family[samples_data['selected'] == 1].tolist()\n",
    "vis_data.plot_data('data/d_matrices/tsne_2_all.txt', families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data.plot_data('data/d_matrices/tsne_3_1209.txt', families)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the 3d representation generated through tSNE does not provide a clear view of the data. We have more success with a 3d representation obtained using PCA for dimensionality reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data.plot_data('data/d_matrices/pca_3_1209.txt', families)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the classification provided by the AV data with the result of our clustering, plotted over the same dimensionality reduced data points.\n",
    "\n",
    "Here, the color of the points will reflect the cluster in which they are assigned by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data.plot_data('data/d_matrices/tsne_2_all.txt', clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data.plot_data('data/d_matrices/pca_3_1209.txt', clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
