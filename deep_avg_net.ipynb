{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deep Averaging Networks for malware classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will experiment with the concept of Deep Averaging Networks in our malware classification setting.\n",
    "\n",
    "Let's start by loading some packages necessary for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import defaultdict, Counter\n",
    "from preprocessing import pp_action\n",
    "from helpers import loader_tfidf\n",
    "from utilities import constants\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as ply\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('config.json', 'r'))\n",
    "uuids_family = json.load(open(os.path.join(constants.dir_d, constants.json_labels), 'r'))\n",
    "words = json.load(open(os.path.join(constants.dir_d, constants.json_words), 'r'))\n",
    "ply.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "Select a subset of the original dataset. Then the selected subset will be split into a training and a testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data = pp_action.pre_process(config)\n",
    "pp_action.split_show_data(samples_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uuids = samples_data.index[samples_data['selected'] == 1].tolist()\n",
    "x_train = samples_data.index[samples_data['train'] == 1].tolist()\n",
    "x_test = samples_data.index[samples_data['test'] == 1].tolist()\n",
    "y_train = samples_data.fam_num[samples_data['train'] == 1].tolist()\n",
    "y_test = samples_data.fam_num[samples_data['test'] == 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Since the DAN required a very considerable amount fo time for the training processes we will try reducing the dimensionality of the dataset.\n",
    "\n",
    "We would also like this approach to be scalable to the entire balanced dataset so we will load sparse representations of the data vectors.\n",
    "\n",
    "To achieve this we will use Singular Value Decomposition in order to operate on the sparse vectros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = loader_tfidf.load_tfidf(config, x_train, dense=False, ordered=True)\n",
    "test = loader_tfidf.load_tfidf(config, x_test, dense=False, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd.fit(train)\n",
    "print(tsvd.explained_variance_ratio_.sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = tsvd.transform(train)\n",
    "X_test = tsvd.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = X_train[i] / X_train.shape[1]\n",
    "X_train = X_train.T\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i] = X_test[i] / X_test.shape[1]\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels pre-processing\n",
    "\n",
    "We will initially convert the true labels into a one-hot vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(set(y_train))\n",
    "n_classes = len(classes)\n",
    "\n",
    "classes_dict = dict(zip(classes, range(n_classes)))\n",
    "y_train = [classes_dict[i] for i in y_train]\n",
    "y_test = [classes_dict[i] for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "Y_train = lb.fit_transform(y_train).T\n",
    "Y_test = lb.fit_transform(y_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Hyper-parameters\n",
    "\n",
    "Let's set the hyper-paramters, we will try to start with a fast network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "n_epochs = 1500\n",
    "minibatch_size = 256\n",
    "n_h_layers = 3\n",
    "# n_h_layers = 2\n",
    "# ls = [[24,X_train.shape[0]], [24,1], [12,24], [12,1], [Y_train.shape[0],12], [Y_train.shape[0],1]]\n",
    "ls = [[100,X_train.shape[0]], [100,1], [80,100], [80,1], [Y_train.shape[0],80], [Y_train.shape[0],1]]\n",
    "# ls = [[12,X_train.shape[0]], [12,1], [Y_train.shape[0],12], [Y_train.shape[0],1]]\n",
    "keep_probs = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model definition\n",
    "\n",
    "At each step the vectors will go through a softmax function.\n",
    "\n",
    "First let's define some placeholders for the input X and the labels Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_ph(n_feats, n_classes):\n",
    "    with tf.device('/gpu:0'):\n",
    "        X = tf.placeholder(dtype=tf.float32, shape=(n_feats, None))\n",
    "        Y = tf.placeholder(dtype=tf.float32, shape=(n_classes, None))\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        return X,Y, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the wiehgts using the Xavier intialization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(n_layers, layer_sizes):\n",
    "    params = {}\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        for i in range(n_layers):\n",
    "            Wn = 'W{}'.format(i)\n",
    "            bn = 'b{}'.format(i)\n",
    "            \n",
    "            params[Wn] = tf.get_variable(\n",
    "                Wn, \n",
    "                layer_sizes[i * 2], \n",
    "                initializer = tf.contrib.layers.xavier_initializer(seed = 1)\n",
    "            )\n",
    "            \n",
    "            params[bn] = tf.get_variable(\n",
    "                bn, \n",
    "                layer_sizes[(i * 2) + 1],\n",
    "                initializer = tf.zeros_initializer()\n",
    "            )\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fwd(X, params, keep_prob):\n",
    "    Zn = None\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        An = X\n",
    "        \n",
    "        for i in range(n_h_layers):\n",
    "            Wn = 'W{}'.format(i)\n",
    "            bn = 'b{}'.format(i)\n",
    "            \n",
    "            Zn = tf.add(tf.matmul(params[Wn], An), params[bn])\n",
    "            An = tf.nn.dropout(tf.nn.relu(Zn), keep_prob)\n",
    "            \n",
    "    return Zn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Zn, Y):\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        logits = tf.transpose(Zn)\n",
    "        labels = tf.transpose(Y)\n",
    "        \n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finally the DAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dan(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs, minibatch_size, n_h_layers, layers, keep_probs):\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        X, Y, keep_prob = init_ph(X_train.shape[0], Y_train.shape[0])\n",
    "        \n",
    "        params = init_weights(n_h_layers, layers)\n",
    "        \n",
    "        Z = fwd(X, params, keep_prob)\n",
    "        \n",
    "        cost = compute_cost(Z, Y)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        \n",
    "            num_minibatches = int(X_train.shape[1] / minibatch_size)\n",
    "\n",
    "            sess.run(init)\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                epoch_cost = 0.\n",
    "                \n",
    "                minibatch_idxs = np.random.permutation(X_train.shape[1])\n",
    "                \n",
    "                for i in range(num_minibatches):\n",
    "                   \n",
    "\n",
    "                    minibatch_X = np.take(\n",
    "                        X_train,\n",
    "                        minibatch_idxs[i * minibatch_size : (i + 1) * minibatch_size], \n",
    "                        axis=1\n",
    "                    )\n",
    "                    minibatch_Y = np.take(\n",
    "                        Y_train, \n",
    "                        minibatch_idxs[i * minibatch_size : (i + 1) * minibatch_size], \n",
    "                        axis=1\n",
    "                    )\n",
    "\n",
    "                    _ , minibatch_cost = sess.run(\n",
    "                        [optimizer, cost], \n",
    "                        feed_dict={\n",
    "                            X: minibatch_X, \n",
    "                            Y: minibatch_Y,\n",
    "                            keep_prob: keep_probs\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "                if epoch % 100 == 0:\n",
    "                    print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "                if epoch % 5 == 0:\n",
    "                    costs.append(epoch_cost)\n",
    "\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            \n",
    "            tr_acc =  accuracy.eval({X: X_train, Y: Y_train, keep_prob: 1.0})\n",
    "            ts_acc = accuracy.eval({X: X_test, Y: Y_test, keep_prob: 1.0})\n",
    "            \n",
    "            print (\"Train Accuracy:\",tr_acc)\n",
    "            print (\"Test Accuracy:\", ts_acc)\n",
    "\n",
    "\n",
    "        return params, costs, tr_acc, ts_acc\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1)\n",
    "costs = []\n",
    "\n",
    "tf.reset_default_graph()\n",
    "parameters, cost_list, tr_acc, ts_acc = dan(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    learning_rate,\n",
    "    n_epochs,\n",
    "    minibatch_size,\n",
    "    n_h_layers,\n",
    "    ls,\n",
    "    keep_probs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    x = np.arange(len(costs)),\n",
    "    y = costs\n",
    ")\n",
    "ply.iplot([trace], filename='costs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
