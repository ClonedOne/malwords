{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deep Averaging Networks for malware classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will experiment with the concept of Deep Averaging Networks in our malware classification setting.\n",
    "\n",
    "Let's start by loading some packages necessary for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.externals import joblib\n",
    "from preprocessing import pp_action\n",
    "from helpers import loader_tfidf\n",
    "from utilities import constants\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as ply\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = json.load(open('config.json', 'r'))\n",
    "uuids_family = json.load(open(os.path.join(constants.dir_d, constants.json_labels), 'r'))\n",
    "words = json.load(open(os.path.join(constants.dir_d, constants.json_words), 'r'))\n",
    "ply.init_notebook_mode(connected=True)\n",
    "load_batch_size = 1100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "Select a subset of the original dataset. Then the selected subset will be split into a training and a testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose the subset of data to workon on:\n",
      "l for all labeled samples\n",
      "k for samples of families mydoom, gepys, lamer, neshta, bladabindi, flystudio, eorezo\n",
      "s for 8 samples of families mydoom, gepys, bladabindi, flystudio\n",
      "f for a single family\n",
      "b for a balanced subset of samples\n",
      "q to quit\n",
      "b\n",
      "\n",
      "Would you like to compute the Jensen-Shannon distance matrix for the chosen data? [y/n]\n",
      "It may take very long, depending on the number of selected samples\n",
      "n\n",
      "\n",
      "20007 train samples belonging to 65 malware families\n",
      "Malware family:      multiplug       Number of samples:  748  \n",
      "Malware family:     installcore      Number of samples:  724  \n",
      "Malware family:       firseria       Number of samples:  720  \n",
      "Malware family:      outbrowse       Number of samples:  712  \n",
      "Malware family:       virlock        Number of samples:  707  \n",
      "Malware family:      loadmoney       Number of samples:  706  \n",
      "Malware family:        sality        Number of samples:  703  \n",
      "Malware family:      browsefox       Number of samples:  701  \n",
      "Malware family:       allaple        Number of samples:  698  \n",
      "Malware family:         mira         Number of samples:  695  \n",
      "Malware family:      softpulse       Number of samples:  694  \n",
      "Malware family:        vobfus        Number of samples:  692  \n",
      "Malware family:        sytro         Number of samples:  685  \n",
      "Malware family:        parite        Number of samples:  679  \n",
      "Malware family:        upatre        Number of samples:  678  \n",
      "Malware family:        virut         Number of samples:  677  \n",
      "Malware family:         klez         Number of samples:  541  \n",
      "Malware family:        dinwod        Number of samples:  517  \n",
      "Malware family:      vtflooder       Number of samples:  413  \n",
      "Malware family:        expiro        Number of samples:  375  \n",
      "Malware family:       icloader       Number of samples:  310  \n",
      "Malware family:        wapomi        Number of samples:  301  \n",
      "Malware family:        ramnit        Number of samples:  299  \n",
      "Malware family:        ircbot        Number of samples:  271  \n",
      "Malware family:    installmonster    Number of samples:  256  \n",
      "Malware family:         zbot         Number of samples:  254  \n",
      "Malware family:        autoit        Number of samples:  248  \n",
      "Malware family:        domaiq        Number of samples:  243  \n",
      "Malware family:        ipamor        Number of samples:  242  \n",
      "Malware family:    downloadguide     Number of samples:  236  \n",
      "Malware family:     installerex      Number of samples:  225  \n",
      "Malware family:      bladabindi      Number of samples:  187  \n",
      "Malware family:         delf         Number of samples:  184  \n",
      "Malware family:        ibryte        Number of samples:  182  \n",
      "Malware family:        eorezo        Number of samples:  174  \n",
      "Malware family:        neshta        Number of samples:  162  \n",
      "Malware family:        vilsel        Number of samples:  161  \n",
      "Malware family:      crossrider      Number of samples:  155  \n",
      "Malware family:         zusy         Number of samples:  148  \n",
      "Malware family:      amonetize       Number of samples:  147  \n",
      "Malware family:        cosmu         Number of samples:  138  \n",
      "Malware family:        loring        Number of samples:  133  \n",
      "Malware family:     airinstaller     Number of samples:  131  \n",
      "Malware family:    downloadadmin     Number of samples:  128  \n",
      "Malware family:        urelas        Number of samples:  123  \n",
      "Malware family:      convertad       Number of samples:  121  \n",
      "Malware family:        simbot        Number of samples:  115  \n",
      "Malware family:        yuner         Number of samples:  113  \n",
      "Malware family:  downloadassistant   Number of samples:  110  \n",
      "Malware family:        imali         Number of samples:  107  \n",
      "Malware family:       dealply        Number of samples:  106  \n",
      "Malware family:       gamarue        Number of samples:  105  \n",
      "Malware family:        mydoom        Number of samples:  104  \n",
      "Malware family:        lydra         Number of samples:  104  \n",
      "Malware family:        memery        Number of samples:   99  \n",
      "Malware family:      softcnapp       Number of samples:   97  \n",
      "Malware family:       dlhelper       Number of samples:   95  \n",
      "Malware family:        picsys        Number of samples:   93  \n",
      "Malware family:        lamer         Number of samples:   87  \n",
      "Malware family:        berbew        Number of samples:   84  \n",
      "Malware family:       bundlore       Number of samples:   77  \n",
      "Malware family:       filetour       Number of samples:   76  \n",
      "Malware family:        shipup        Number of samples:   75  \n",
      "Malware family:        hotbar        Number of samples:   72  \n",
      "Malware family:         chir         Number of samples:   64  \n",
      "\n",
      "4288 dev samples belonging to 65 malware families\n",
      "Malware family:        vobfus        Number of samples:  171  \n",
      "Malware family:        upatre        Number of samples:  169  \n",
      "Malware family:        virut         Number of samples:  166  \n",
      "Malware family:         mira         Number of samples:  163  \n",
      "Malware family:        sytro         Number of samples:  162  \n",
      "Malware family:        sality        Number of samples:  154  \n",
      "Malware family:       virlock        Number of samples:  152  \n",
      "Malware family:       allaple        Number of samples:  148  \n",
      "Malware family:        parite        Number of samples:  147  \n",
      "Malware family:      softpulse       Number of samples:  146  \n",
      "Malware family:       firseria       Number of samples:  143  \n",
      "Malware family:      browsefox       Number of samples:  141  \n",
      "Malware family:      outbrowse       Number of samples:  132  \n",
      "Malware family:      loadmoney       Number of samples:  129  \n",
      "Malware family:      multiplug       Number of samples:  128  \n",
      "Malware family:     installcore      Number of samples:  115  \n",
      "Malware family:        dinwod        Number of samples:  111  \n",
      "Malware family:         klez         Number of samples:  111  \n",
      "Malware family:        expiro        Number of samples:   90  \n",
      "Malware family:        ramnit        Number of samples:   78  \n",
      "Malware family:        wapomi        Number of samples:   77  \n",
      "Malware family:      vtflooder       Number of samples:   76  \n",
      "Malware family:        ircbot        Number of samples:   67  \n",
      "Malware family:       icloader       Number of samples:   63  \n",
      "Malware family:         zbot         Number of samples:   59  \n",
      "Malware family:        ipamor        Number of samples:   57  \n",
      "Malware family:        autoit        Number of samples:   55  \n",
      "Malware family:    installmonster    Number of samples:   55  \n",
      "Malware family:     installerex      Number of samples:   53  \n",
      "Malware family:        eorezo        Number of samples:   51  \n",
      "Malware family:    downloadguide     Number of samples:   48  \n",
      "Malware family:        ibryte        Number of samples:   45  \n",
      "Malware family:        domaiq        Number of samples:   44  \n",
      "Malware family:      bladabindi      Number of samples:   40  \n",
      "Malware family:        simbot        Number of samples:   39  \n",
      "Malware family:        loring        Number of samples:   39  \n",
      "Malware family:         delf         Number of samples:   34  \n",
      "Malware family:        vilsel        Number of samples:   33  \n",
      "Malware family:         zusy         Number of samples:   32  \n",
      "Malware family:      softcnapp       Number of samples:   31  \n",
      "Malware family:      amonetize       Number of samples:   30  \n",
      "Malware family:      crossrider      Number of samples:   30  \n",
      "Malware family:    downloadadmin     Number of samples:   29  \n",
      "Malware family:        cosmu         Number of samples:   28  \n",
      "Malware family:        urelas        Number of samples:   27  \n",
      "Malware family:        imali         Number of samples:   26  \n",
      "Malware family:        memery        Number of samples:   24  \n",
      "Malware family:        neshta        Number of samples:   24  \n",
      "Malware family:       dealply        Number of samples:   23  \n",
      "Malware family:     airinstaller     Number of samples:   22  \n",
      "Malware family:        lamer         Number of samples:   22  \n",
      "Malware family:        mydoom        Number of samples:   21  \n",
      "Malware family:        berbew        Number of samples:   21  \n",
      "Malware family:        lydra         Number of samples:   21  \n",
      "Malware family:        yuner         Number of samples:   20  \n",
      "Malware family:         chir         Number of samples:   19  \n",
      "Malware family:  downloadassistant   Number of samples:   18  \n",
      "Malware family:       gamarue        Number of samples:   18  \n",
      "Malware family:       bundlore       Number of samples:   18  \n",
      "Malware family:      convertad       Number of samples:   17  \n",
      "Malware family:        picsys        Number of samples:   17  \n",
      "Malware family:        shipup        Number of samples:   17  \n",
      "Malware family:       filetour       Number of samples:   16  \n",
      "Malware family:       dlhelper       Number of samples:   13  \n",
      "Malware family:        hotbar        Number of samples:   13  \n",
      "\n",
      "4287 test samples belonging to 65 malware families\n",
      "Malware family:        parite        Number of samples:  174  \n",
      "Malware family:      loadmoney       Number of samples:  165  \n",
      "Malware family:     installcore      Number of samples:  161  \n",
      "Malware family:      softpulse       Number of samples:  160  \n",
      "Malware family:      browsefox       Number of samples:  158  \n",
      "Malware family:        virut         Number of samples:  157  \n",
      "Malware family:      outbrowse       Number of samples:  156  \n",
      "Malware family:       allaple        Number of samples:  154  \n",
      "Malware family:        upatre        Number of samples:  153  \n",
      "Malware family:        sytro         Number of samples:  153  \n",
      "Malware family:        sality        Number of samples:  143  \n",
      "Malware family:         mira         Number of samples:  142  \n",
      "Malware family:       virlock        Number of samples:  141  \n",
      "Malware family:        vobfus        Number of samples:  137  \n",
      "Malware family:       firseria       Number of samples:  124  \n",
      "Malware family:      multiplug       Number of samples:  124  \n",
      "Malware family:        dinwod        Number of samples:  112  \n",
      "Malware family:         klez         Number of samples:   97  \n",
      "Malware family:        expiro        Number of samples:   79  \n",
      "Malware family:      vtflooder       Number of samples:   74  \n",
      "Malware family:        wapomi        Number of samples:   71  \n",
      "Malware family:    installmonster    Number of samples:   63  \n",
      "Malware family:        ramnit        Number of samples:   62  \n",
      "Malware family:       icloader       Number of samples:   61  \n",
      "Malware family:         zbot         Number of samples:   56  \n",
      "Malware family:        ipamor        Number of samples:   54  \n",
      "Malware family:        autoit        Number of samples:   54  \n",
      "Malware family:        ircbot        Number of samples:   51  \n",
      "Malware family:    downloadguide     Number of samples:   49  \n",
      "Malware family:        domaiq        Number of samples:   45  \n",
      "Malware family:     installerex      Number of samples:   44  \n",
      "Malware family:        neshta        Number of samples:   44  \n",
      "Malware family:        loring        Number of samples:   44  \n",
      "Malware family:      bladabindi      Number of samples:   42  \n",
      "Malware family:        eorezo        Number of samples:   41  \n",
      "Malware family:      amonetize       Number of samples:   38  \n",
      "Malware family:        urelas        Number of samples:   37  \n",
      "Malware family:        cosmu         Number of samples:   36  \n",
      "Malware family:        vilsel        Number of samples:   36  \n",
      "Malware family:     airinstaller     Number of samples:   32  \n",
      "Malware family:      convertad       Number of samples:   31  \n",
      "Malware family:         delf         Number of samples:   31  \n",
      "Malware family:        ibryte        Number of samples:   30  \n",
      "Malware family:    downloadadmin     Number of samples:   30  \n",
      "Malware family:         zusy         Number of samples:   29  \n",
      "Malware family:        simbot        Number of samples:   29  \n",
      "Malware family:      crossrider      Number of samples:   28  \n",
      "Malware family:      softcnapp       Number of samples:   28  \n",
      "Malware family:        lamer         Number of samples:   27  \n",
      "Malware family:       dealply        Number of samples:   25  \n",
      "Malware family:       gamarue        Number of samples:   24  \n",
      "Malware family:        memery        Number of samples:   22  \n",
      "Malware family:        hotbar        Number of samples:   22  \n",
      "Malware family:        lydra         Number of samples:   22  \n",
      "Malware family:        imali         Number of samples:   21  \n",
      "Malware family:        mydoom        Number of samples:   21  \n",
      "Malware family:       bundlore       Number of samples:   20  \n",
      "Malware family:         chir         Number of samples:   18  \n",
      "Malware family:  downloadassistant   Number of samples:   18  \n",
      "Malware family:        picsys        Number of samples:   17  \n",
      "Malware family:        yuner         Number of samples:   16  \n",
      "Malware family:       dlhelper       Number of samples:   16  \n",
      "Malware family:        shipup        Number of samples:   14  \n",
      "Malware family:        berbew        Number of samples:   13  \n",
      "Malware family:       filetour       Number of samples:   11  \n",
      "\n",
      "\n",
      "         family  fam_num  selected    train     dev    test\n",
      "count     58180  58180.0   28582.0  20007.0  4288.0  4287.0\n",
      "unique     1270   1270.0       NaN      NaN     NaN     NaN\n",
      "top     allaple     42.0       NaN      NaN     NaN     NaN\n",
      "freq      10569  10569.0       NaN      NaN     NaN     NaN\n",
      "mean        NaN      NaN       1.0      1.0     1.0     1.0\n",
      "std         NaN      NaN       0.0      0.0     0.0     0.0\n",
      "min         NaN      NaN       1.0      1.0     1.0     1.0\n",
      "25%         NaN      NaN       1.0      1.0     1.0     1.0\n",
      "50%         NaN      NaN       1.0      1.0     1.0     1.0\n",
      "75%         NaN      NaN       1.0      1.0     1.0     1.0\n",
      "max         NaN      NaN       1.0      1.0     1.0     1.0\n"
     ]
    }
   ],
   "source": [
    "samples_data = pp_action.pre_process(config)\n",
    "pp_action.split_show_data(samples_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uuids = samples_data.index[samples_data['selected'] == 1].tolist()\n",
    "x_train = samples_data.index[samples_data['train'] == 1].tolist()\n",
    "x_dev = samples_data.index[samples_data['dev'] == 1].tolist()\n",
    "x_test = samples_data.index[samples_data['test'] == 1].tolist()\n",
    "y_train = samples_data.fam_num[samples_data['train'] == 1].tolist()\n",
    "y_dev = samples_data.fam_num[samples_data['dev'] == 1].tolist()\n",
    "y_test = samples_data.fam_num[samples_data['test'] == 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Since the DAN required a very considerable amount fo time for the training processes we will try reducing the dimensionality of the dataset.\n",
    "\n",
    "We would also like this approach to be scalable to the entire balanced dataset so we will load sparse representations of the data vectors.\n",
    "\n",
    "To achieve this we will use the Principal Component Analysis in order to operate on the sparse vectros. Let's define two helper functions first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_pca(config, i_pca, samples, load_batch_size):\n",
    "    t = 0\n",
    "    \n",
    "    while t < len(samples):\n",
    "        data = loader_tfidf.load_tfidf(config, samples[t : t + load_batch_size], dense=True, ordered=False)\n",
    "        t += load_batch_size\n",
    "\n",
    "        i_pca.partial_fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_data(config, i_pca, samples, load_batch_size):\n",
    "    new_data = [] \n",
    "    t = 0\n",
    "    \n",
    "    while t < len(samples):\n",
    "        data = loader_tfidf.load_tfidf(config, samples[t : t + load_batch_size], dense=True, ordered=True)\n",
    "        t += load_batch_size\n",
    "\n",
    "        new_data.append(i_pca.transform(data))\n",
    "        \n",
    "    return np.concatenate(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_pca = IncrementalPCA(n_components=1024, batch_size=load_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the PCA algorithm incrementally only on the trainining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n"
     ]
    }
   ],
   "source": [
    "train_pca(config, i_pca, random.sample(x_train, len(x_train)), load_batch_size)\n",
    "joblib.dump(i_pca, 'temp_pca_1000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72908727985\n"
     ]
    }
   ],
   "source": [
    "print(i_pca.explained_variance_ratio_.sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# or directly load the trained PCA model if available\n",
    "i_pca = joblib.load('temp_pca.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will use the trained algorithm to (incrementally) transform all the data vectors. This will allow us to transform larger dataset than what would fit in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 207 documents\n",
      "(207, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 988 documents\n",
      "(988, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 1100 documents\n",
      "(1100, 297360)\n",
      "Loading Tf-Idf of 987 documents\n",
      "(987, 297360)\n"
     ]
    }
   ],
   "source": [
    "X_train = transform_data(config, i_pca, x_train, load_batch_size)\n",
    "X_dev = transform_data(config, i_pca, x_dev, load_batch_size)\n",
    "X_test = transform_data(config, i_pca, x_test, load_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = X_train[i] / X_train.shape[1]\n",
    "X_train = X_train.T\n",
    "\n",
    "for i in range(X_dev.shape[0]):\n",
    "    X_dev[i] = X_dev[i] / X_dev.shape[1]\n",
    "X_dev = X_dev.T\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i] = X_test[i] / X_test.shape[1]\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels pre-processing\n",
    "\n",
    "We will initially convert the true labels into a one-hot vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(set(y_train))\n",
    "n_classes = len(classes)\n",
    "\n",
    "classes_dict = dict(zip(classes, range(n_classes)))\n",
    "y_train = [classes_dict[i] for i in y_train]\n",
    "y_dev = [classes_dict[i] for i in y_dev]\n",
    "y_test = [classes_dict[i] for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "Y_train = lb.fit_transform(y_train).T\n",
    "Y_dev = lb.fit_transform(y_dev).T\n",
    "Y_test = lb.fit_transform(y_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1024, 20007)\n",
      "Y_train shape: (65, 20007)\n",
      "X_dev shape: (1024, 4288)\n",
      "Y_dev shape: (65, 4288)\n",
      "X_test shape: (1024, 4287)\n",
      "Y_test shape: (65, 4287)\n"
     ]
    }
   ],
   "source": [
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_dev shape: \" + str(X_dev.shape))\n",
    "print (\"Y_dev shape: \" + str(Y_dev.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Hyper-parameters\n",
    "\n",
    "Let's set the hyper-paramters, we will try to start with a fast network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "n_epochs = 1500\n",
    "minibatch_size = 512\n",
    "n_h_layers = 2\n",
    "# ls = [[128,X_train.shape[0]], [128,1], [128,128], [128,1], [Y_train.shape[0],128], [Y_train.shape[0],1]]\n",
    "# ls = [[512,X_train.shape[0]], [512,1], [256, 512], [256,1], [128,256], [128,1], [Y_train.shape[0],128], [Y_train.shape[0],1]]\n",
    "ls = [[128,X_train.shape[0]], [128,1], [Y_train.shape[0],128], [Y_train.shape[0],1]]\n",
    "keep_probs = 0.8\n",
    "reg = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model definition\n",
    "\n",
    "At each step the vectors will go through a softmax function.\n",
    "\n",
    "First let's define some placeholders for the input X and the labels Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_ph(n_feats, n_classes):\n",
    "    with tf.device('/gpu:0'):\n",
    "        X = tf.placeholder(dtype=tf.float32, shape=(n_feats, None))\n",
    "        Y = tf.placeholder(dtype=tf.float32, shape=(n_classes, None))\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        return X,Y, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the wiehgts using the Xavier intialization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(n_layers, layer_sizes):\n",
    "    params = {}\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        for i in range(n_layers):\n",
    "            Wn = 'W{}'.format(i)\n",
    "            bn = 'b{}'.format(i)\n",
    "            \n",
    "            params[Wn] = tf.get_variable(\n",
    "                Wn, \n",
    "                layer_sizes[i * 2], \n",
    "                initializer = tf.contrib.layers.xavier_initializer(seed = 1)\n",
    "            )\n",
    "            \n",
    "            params[bn] = tf.get_variable(\n",
    "                bn, \n",
    "                layer_sizes[(i * 2) + 1],\n",
    "                initializer = tf.zeros_initializer()\n",
    "            )\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fwd(X, params, keep_prob):\n",
    "    Zn = None\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        An = X\n",
    "#         An = tf.nn.dropout(X, keep_prob)\n",
    "        \n",
    "        for i in range(n_h_layers):\n",
    "            Wn = 'W{}'.format(i)\n",
    "            bn = 'b{}'.format(i)\n",
    "            \n",
    "            Zn = tf.add(tf.matmul(params[Wn], An), params[bn])\n",
    "            An = tf.nn.dropout(tf.nn.relu(Zn), keep_prob)\n",
    "            \n",
    "    return Zn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Zn, Y, reg, params, n_layers):\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        logits = tf.transpose(Zn)\n",
    "        labels = tf.transpose(Y)\n",
    "        \n",
    "        regularization = 0.0\n",
    "        for i in range(n_layers):\n",
    "            Wn = 'W{}'.format(i)\n",
    "            regularization += tf.nn.l2_loss(params[Wn])\n",
    "        \n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)) + (reg * regularization)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finally the DAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dan(X_train, Y_train, X_test, Y_test, l_rate, n_epochs, minibatch_size, n_h_layers, layers, k_prob, reg):\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        X, Y, keep_prob = init_ph(X_train.shape[0], Y_train.shape[0])\n",
    "        \n",
    "        params = init_weights(n_h_layers, layers)\n",
    "        \n",
    "        Z = fwd(X, params, keep_prob)\n",
    "        \n",
    "        cost = compute_cost(Z, Y, reg, params, n_h_layers)\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        learning_rate = tf.train.exponential_decay(l_rate, global_step, 100000, 0.99)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = l_rate).minimize(cost, global_step=global_step)\n",
    "    \n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate = l_rate).minimize(cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()  \n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        \n",
    "            num_minibatches = int(X_train.shape[1] / minibatch_size)\n",
    "\n",
    "            sess.run(init)\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                epoch_cost = 0.\n",
    "                \n",
    "                minibatch_idxs = np.random.permutation(X_train.shape[1])\n",
    "                \n",
    "                for i in range(num_minibatches):\n",
    "                   \n",
    "\n",
    "                    minibatch_X = np.take(\n",
    "                        X_train,\n",
    "                        minibatch_idxs[i * minibatch_size : (i + 1) * minibatch_size], \n",
    "                        axis=1\n",
    "                    )\n",
    "                    minibatch_Y = np.take(\n",
    "                        Y_train, \n",
    "                        minibatch_idxs[i * minibatch_size : (i + 1) * minibatch_size], \n",
    "                        axis=1\n",
    "                    )\n",
    "\n",
    "                    _ , minibatch_cost = sess.run(\n",
    "                        [optimizer, cost], \n",
    "                        feed_dict={\n",
    "                            X: minibatch_X, \n",
    "                            Y: minibatch_Y,\n",
    "                            keep_prob: k_prob\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "                if epoch % 100 == 0:\n",
    "                    print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "                    print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train, keep_prob: 1.0}))\n",
    "                    print (\"Dev Accuracy:\", accuracy.eval({X: X_dev, Y: Y_dev, keep_prob: 1.0}))\n",
    "                    print (\"Learning Rate:\", learning_rate.eval())\n",
    "                    print (\"\")\n",
    "                    \n",
    "                if epoch % 5 == 0:\n",
    "                    costs.append(epoch_cost)\n",
    "\n",
    "\n",
    "            tr_acc =  accuracy.eval({X: X_train, Y: Y_train, keep_prob: 1.0})\n",
    "            dv_acc = accuracy.eval({X: X_dev, Y: Y_dev, keep_prob: 1.0})\n",
    "            \n",
    "            print (\"Train Accuracy:\",tr_acc)\n",
    "            print (\"Dev Accuracy:\", dv_acc)\n",
    "\n",
    "\n",
    "        return params, costs, tr_acc, dv_acc\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 4.015227\n",
      "Train Accuracy: 0.641225\n",
      "Dev Accuracy: 0.631996\n",
      "Learning Rate: 0.000999996\n",
      "\n",
      "Cost after epoch 100: 0.302301\n",
      "Train Accuracy: 0.925925\n",
      "Dev Accuracy: 0.90555\n",
      "Learning Rate: 0.000999604\n",
      "\n",
      "Cost after epoch 200: 0.210804\n",
      "Train Accuracy: 0.944419\n",
      "Dev Accuracy: 0.91861\n",
      "Learning Rate: 0.000999212\n",
      "\n",
      "Cost after epoch 300: 0.168607\n",
      "Train Accuracy: 0.957164\n",
      "Dev Accuracy: 0.927239\n",
      "Learning Rate: 0.000998821\n",
      "\n",
      "Cost after epoch 400: 0.140943\n",
      "Train Accuracy: 0.961713\n",
      "Dev Accuracy: 0.93097\n",
      "Learning Rate: 0.000998429\n",
      "\n",
      "Cost after epoch 500: 0.118579\n",
      "Train Accuracy: 0.96801\n",
      "Dev Accuracy: 0.934002\n",
      "Learning Rate: 0.000998038\n",
      "\n",
      "Cost after epoch 600: 0.105775\n",
      "Train Accuracy: 0.972609\n",
      "Dev Accuracy: 0.9368\n",
      "Learning Rate: 0.000997647\n",
      "\n",
      "Cost after epoch 700: 0.093336\n",
      "Train Accuracy: 0.976358\n",
      "Dev Accuracy: 0.937034\n",
      "Learning Rate: 0.000997256\n",
      "\n",
      "Cost after epoch 800: 0.082394\n",
      "Train Accuracy: 0.979956\n",
      "Dev Accuracy: 0.9382\n",
      "Learning Rate: 0.000996865\n",
      "\n",
      "Cost after epoch 900: 0.073736\n",
      "Train Accuracy: 0.983505\n",
      "Dev Accuracy: 0.940065\n",
      "Learning Rate: 0.000996475\n",
      "\n",
      "Cost after epoch 1000: 0.066503\n",
      "Train Accuracy: 0.983255\n",
      "Dev Accuracy: 0.938899\n",
      "Learning Rate: 0.000996084\n",
      "\n",
      "Cost after epoch 1100: 0.060771\n",
      "Train Accuracy: 0.986454\n",
      "Dev Accuracy: 0.941231\n",
      "Learning Rate: 0.000995694\n",
      "\n",
      "Cost after epoch 1200: 0.056803\n",
      "Train Accuracy: 0.987504\n",
      "Dev Accuracy: 0.941231\n",
      "Learning Rate: 0.000995304\n",
      "\n",
      "Cost after epoch 1300: 0.051654\n",
      "Train Accuracy: 0.988253\n",
      "Dev Accuracy: 0.941231\n",
      "Learning Rate: 0.000994914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(1)\n",
    "costs = []\n",
    "\n",
    "tf.reset_default_graph()\n",
    "parameters, cost_list, tr_acc, ts_acc = dan(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    learning_rate,\n",
    "    n_epochs,\n",
    "    minibatch_size,\n",
    "    n_h_layers,\n",
    "    ls,\n",
    "    keep_probs,\n",
    "    reg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    x = np.arange(len(costs)),\n",
    "    y = costs\n",
    ")\n",
    "ply.iplot([trace], filename='costs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
