{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inspecting the content of possibly misclassified samples\n",
    "\n",
    "\n",
    "After performin the clustering phase we compared the results with a baseline clustering provided by AV labels. \n",
    "\n",
    "From this comparison it was clear that there were some malware families which where classified in the same way by both our clustering and the AVs.\n",
    "\n",
    "At the same time, however, there are groups of samples which result close in our feature space while being cathegorized as belonging to different families by the AVs.\n",
    "\n",
    "We would like to inspect this samples to better understand why they were classified differently from the AV baseline.\n",
    "\n",
    "Let's start by importing some useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from sklearn.externals import joblib\n",
    "from preprocessing import pp_action\n",
    "from utilities import db_manager\n",
    "from utilities import constants\n",
    "import plotly.offline as ply\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('config.json', 'r'))\n",
    "uuids_family = json.load(open(os.path.join(constants.dir_d, constants.json_labels), 'r'))\n",
    "words = json.load(open(os.path.join(constants.dir_d, constants.json_words), 'r'))\n",
    "ply.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data = pp_action.pre_process(config)\n",
    "pp_action.split_show_data(samples_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uuids = samples_data.index[samples_data['selected'] == 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the labels and clustering results files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = json.load(open('data/labels.json', 'r'))\n",
    "inv_labels = json.load(open('data/inverted_labels.json', 'r'))\n",
    "\n",
    "clustering = json.load(open('data/d_clusterings/clustering_hdbscan_cosine_1209.json', 'r'))\n",
    "uuid_md5 = db_manager.acquire_malware_file_dict_full(config['dir_db'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_compositions = {i: Counter() for i in sorted(set(clustering.values()))}\n",
    "\n",
    "for i in clustering:\n",
    "    clust_compositions[clustering[i]][labels[i]] += 1\n",
    "\n",
    "for clu in sorted(clust_compositions.keys()):\n",
    "    print('Cluster {}:'.format(clu))\n",
    "    print(clust_compositions[clu].most_common())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverted_clustering = defaultdict(list)\n",
    "for i in clustering:\n",
    "    inverted_clustering[clustering[i]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's isolate the noise cluster, i.e. the samples which the algorithm was unable to fit in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise = inverted_clustering[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster seems composed primarily by samples of the Eorezo and Bladabindi families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_e = []\n",
    "noise_b = []\n",
    "\n",
    "for uuid in noise:\n",
    "    if uuids_family[uuid] == 'eorezo':\n",
    "        noise_e.append(uuid)\n",
    "    elif uuids_family[uuid] == 'bladabindi':\n",
    "        noise_b.append(uuid)\n",
    "\n",
    "noise_e = sorted(noise_e)\n",
    "noise_b = sorted(noise_b)\n",
    "\n",
    "pprint(dict(zip(noise_e[:5], [uuid_md5[i] for i in noise_e[:5]])))\n",
    "pprint(dict(zip(noise_b[:5], [uuid_md5[i] for i in noise_b[:5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for cluster number 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clus4 = inverted_clustering[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it seems this cluster should have been populated primarily by the Flystudio or the Gepys family. However a large number of samples from both Eorezo and Bladabindi are included in this cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus4_e = []\n",
    "clus4_b = []\n",
    "clus4_g = []\n",
    "clus4_f = []\n",
    "\n",
    "for uuid in clus4:\n",
    "    if uuids_family[uuid] == 'eorezo':\n",
    "        clus4_e.append(uuid)\n",
    "    elif uuids_family[uuid] == 'bladabindi':\n",
    "        clus4_b.append(uuid)\n",
    "    elif uuids_family[uuid] == 'gepys':\n",
    "        clus4_g.append(uuid)\n",
    "    elif uuids_family[uuid] == 'flystudio':\n",
    "        clus4_f.append(uuid)\n",
    "\n",
    "\n",
    "clus4_e = sorted(clus4_e)\n",
    "clus4_b = sorted(clus4_b)\n",
    "clus4_g = sorted(clus4_g)\n",
    "clus4_f = sorted(clus4_f)\n",
    "\n",
    "pprint(dict(zip(clus4_e[:5], [uuid_md5[i] for i in clus4_e[:5]])))\n",
    "pprint(dict(zip(clus4_b[:5], [uuid_md5[i] for i in clus4_b[:5]])))\n",
    "pprint(dict(zip(clus4_g[:5], [uuid_md5[i] for i in clus4_g[:5]])))\n",
    "pprint(dict(zip(clus4_f[:5], [uuid_md5[i] for i in clus4_f[:5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Having isolated 5 samples for each 'misclassified' group we can try to inspect each of them individually. Let's start by printing the top ten wordsfor each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_words(config, sample):\n",
    "    tf_idf_file = os.path.join(config['dir_store'], sample)\n",
    "    tf_idf = Counter(json.load(open(tf_idf_file, 'r')))\n",
    "    pprint([i for i in tf_idf.most_common(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_words_grp(config, grp):\n",
    "    for sample in grp:\n",
    "        print(sample)\n",
    "        top_words(config, sample)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-' * 80)\n",
    "print('eorezo')\n",
    "top_words_grp(config, clus4_e[:5])\n",
    "\n",
    "print('-' * 80)\n",
    "print('bladabindi')\n",
    "top_words_grp(config, clus4_b[:5])\n",
    "\n",
    "print('-' * 80)\n",
    "print('gepys')\n",
    "top_words_grp(config, clus4_g[:5])\n",
    "\n",
    "print('-' * 80)\n",
    "print('flystudio')\n",
    "top_words_grp(config, clus4_f[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can understand what words are maintained in PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr_model = joblib.load(os.path.join(constants.dir_d, constants.dir_mod, 'pca_128_1209.pkl')) \n",
    "reduced = np.loadtxt('data/d_matrices/pca_128_1209.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = dict(zip(range(len(words)), sorted(words.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uuid in clus4_e[:5]:\n",
    "    \n",
    "    print(uuid)\n",
    "    \n",
    "    red_vec = reduced[uuids.index(uuid)]\n",
    "    orig_feat_vec = np.dot(red_vec, dr_model.components_)\n",
    "    cent_series = pd.Series(np.abs(orig_feat_vec), index=sorted(words.values()))\n",
    "\n",
    "    print(cent_series.nlargest(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Looking at VirusTotal data\n",
    "\n",
    "Now that we have isolated some problematic samples, let's look at the realted VirusTotal report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Eorezo samples in cluster 4: ', len(clus4_e))\n",
    "for uuid in clus4_e:\n",
    "    md5 = uuid_md5[uuid]\n",
    "    vt = json.load(open(os.path.join(config['dir_vt'], md5), 'r'))\n",
    "    ms_lab = vt['scans']['Microsoft']['result']\n",
    "    ks_lab = vt['scans']['Kaspersky']['result']\n",
    "    fs_lab = vt['scans']['F-Secure']['result']\n",
    "    ca_lab = vt['scans']['ClamAV']['result']\n",
    "    \n",
    "#     print('{:<20} {:<20} {:<20} {:<20}'.format(str(md5), str(ms_lab), str(ks_lab), str(fs_lab)))\n",
    "    print('{:<20} {:<38} {:<30} {:<20}'.format(str(ms_lab), str(ks_lab), str(fs_lab), str(ca_lab)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
