{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN clustering result analysis\n",
    "\n",
    "Let's start by loading up some libraries and static data that may be useful in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utilities import constants\n",
    "import plotly.offline as ply\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = json.load(open('config.json', 'r'))\n",
    "uuids_family = json.load(open(os.path.join(constants.dir_d, constants.json_labels), 'r'))\n",
    "words = json.load(open(os.path.join(constants.dir_d, constants.json_words), 'r'))\n",
    "ply.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "Select a subset of the original dataset. Then the selected subset will be split into a training and a testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import pp_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose the subset of data to workon on:\n",
      "l for all labeled samples\n",
      "k for samples of families mydoom, gepys, lamer, neshta, bladabindi, flystudio, eorezo\n",
      "s for 8 samples of families mydoom, gepys, bladabindi, flystudio\n",
      "f for a single family\n",
      "b for a balanced subset of samples\n",
      "q to quit\n",
      "k\n",
      "\n",
      "967 train samples belonging to 7 malware families\n",
      "Malware family:        eorezo        Number of samples:  219  \n",
      "Malware family:      bladabindi      Number of samples:  218  \n",
      "Malware family:        neshta        Number of samples:  184  \n",
      "Malware family:        mydoom        Number of samples:  116  \n",
      "Malware family:        lamer         Number of samples:  101  \n",
      "Malware family:      flystudio       Number of samples:   70  \n",
      "Malware family:        gepys         Number of samples:   59  \n",
      "\n",
      "242 test samples belonging to 7 malware families\n",
      "Malware family:      bladabindi      Number of samples:   51  \n",
      "Malware family:        eorezo        Number of samples:   47  \n",
      "Malware family:        neshta        Number of samples:   46  \n",
      "Malware family:        lamer         Number of samples:   35  \n",
      "Malware family:        mydoom        Number of samples:   30  \n",
      "Malware family:        gepys         Number of samples:   17  \n",
      "Malware family:      flystudio       Number of samples:   16  \n",
      "\n",
      "\n",
      "         family  fam_num  selected  train   test\n",
      "count     58180  58180.0    1209.0  967.0  242.0\n",
      "unique     1270   1270.0       NaN    NaN    NaN\n",
      "top     allaple     42.0       NaN    NaN    NaN\n",
      "freq      10569  10569.0       NaN    NaN    NaN\n",
      "mean        NaN      NaN       1.0    1.0    1.0\n",
      "std         NaN      NaN       0.0    0.0    0.0\n",
      "min         NaN      NaN       1.0    1.0    1.0\n",
      "25%         NaN      NaN       1.0    1.0    1.0\n",
      "50%         NaN      NaN       1.0    1.0    1.0\n",
      "75%         NaN      NaN       1.0    1.0    1.0\n",
      "max         NaN      NaN       1.0    1.0    1.0\n"
     ]
    }
   ],
   "source": [
    "samples_data = pp_action.pre_process(config)\n",
    "pp_action.split_show_data(samples_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Currently each data vector has approximately 300.000 components. High dimensionality feature vectors usually create problems during the clustering phase.\n",
    "\n",
    "Therefore, before going ahead to clustering the data, we proceed to reduce the dimensionality of the dataset.\n",
    "\n",
    "In this case we will use Principal Components Analysis to transfor our feature vectors in a new, dimensionally smaller, dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dimensionality_reduction import dr_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using PCA\n",
      "Processing documents from 0 to 299\n",
      "Loading Tf-Idf of 300 documents\n",
      "(300, 297360)\n",
      "Processing documents from 300 to 599\n",
      "Loading Tf-Idf of 300 documents\n",
      "(300, 297360)\n",
      "Processing documents from 600 to 899\n",
      "Loading Tf-Idf of 300 documents\n",
      "(300, 297360)\n",
      "Processing documents from 900 to 1199\n",
      "Loading Tf-Idf of 300 documents\n",
      "(300, 297360)\n",
      "Processing documents from 1200 to 1499\n",
      "Loading Tf-Idf of 9 documents\n",
      "(9, 297360)\n",
      "Explained Variance Ratio\n",
      "0.827713758758\n",
      "Transforming documents from 0 to 299\n",
      "Loading Tf-Idf of 300 documents\n",
      "(300, 297360)\n",
      "Transforming documents from 300 to 599\n",
      "Loading Tf-Idf of 300 documents\n",
      "(300, 297360)\n",
      "Transforming documents from 600 to 899\n",
      "Loading Tf-Idf of 300 documents\n",
      "(300, 297360)\n",
      "Transforming documents from 900 to 1199\n",
      "Loading Tf-Idf of 300 documents\n",
      "(300, 297360)\n",
      "Transforming documents from 1200 to 1499\n",
      "Loading Tf-Idf of 9 documents\n",
      "(9, 297360)\n"
     ]
    }
   ],
   "source": [
    "uuids = samples_data.index[samples_data['selected'] == 1].tolist()\n",
    "reduced, dr_model = dr_pca.reduce(config, uuids, 100)\n",
    "\n",
    "# If you had already computed PCA, load it from the disk instead\n",
    "# dr_model = joblib.load(os.path.join(constants.dir_d, constants.dir_mod, 'pca_X_X.pkl)) \n",
    "# reduced = np.loadtxt(matrix_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Once the data dimensionality has been reduced we can proceed with clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering, clu_model = clu_action.cluster(samples_data, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis\n",
    "\n",
    "To better understand the result of the clustering algorithm we would like to see the features characterizing the computed clusters. \n",
    "\n",
    "Since the dataset dimensionality was reduced with PCA before clustering we would need to reverse this step to understand the characteristics of the obtained clusters.\n",
    "\n",
    "To achieve this we will compute the centroids as the average of the data for each cluster and then multiply it by the transposed components matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverted_clustering = defaultdict(list)\n",
    "\n",
    "for i in range(len(uuids)):\n",
    "    inverted_clustering[clustering[i]].append(uuids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_red = pd.DataFrame(reduced, index=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the centroids we will just average the values of the PCA-reduced features of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids = {label : np.zeros(len(reduced[0])) for label in sorted(set(clustering))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for index, vector in data_red.iterrows():\n",
    "    centroids[clustering[i]] += vector.values\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for centroid in centroids:\n",
    "    centroids[centroid] /= len(inverted_clustering[centroid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroid_matrix = []\n",
    "for centroid in sorted(centroids.keys()):\n",
    "    centroid_matrix.append(centroids[centroid])\n",
    "centroid_matrix = np.array(centroid_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the centroid matrix in the PCA space, we can bring it back to its original dimensions by multiplying it with the PCA components matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_orig_fts = np.dot(centroid_matrix, dr_model.components_)\n",
    "centroids_orig_fts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once in the original dimension space we can identify the ten most influencial words for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = dict(zip(range(len(words)), sorted(words.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for centroid in centroids_orig_fts:\n",
    "    cent_series = pd.Series(np.abs(centroid), index=sorted(words.values()))\n",
    "    print(cent_series.nlargest(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be interesting to see which of the initial malware families compose each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust_compositions = {i: Counter() for i in sorted(set(clustering.flatten()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(uuids)):\n",
    "    clust_compositions[clustering[i]][uuids_family[uuids[i]]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clu in sorted(clust_compositions.keys()):\n",
    "    print(clu)\n",
    "    print(clust_compositions[clu].most_common())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_action.visualize(samples_data, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
