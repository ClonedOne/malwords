{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deep Averaging Networks for malware classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will experiment with the concept of Deep Averaging Networks in our malware classification setting.\n",
    "\n",
    "Let's start by loading some packages necessary for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.externals import joblib\n",
    "from preprocessing import pp_action\n",
    "from helpers import loader_tfidf\n",
    "from utilities import constants\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as ply\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('config.json', 'r'))\n",
    "uuids_family = json.load(open(os.path.join(constants.dir_d, constants.json_labels), 'r'))\n",
    "words = json.load(open(os.path.join(constants.dir_d, constants.json_words), 'r'))\n",
    "ply.init_notebook_mode(connected=True)\n",
    "load_batch_size = 1100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "Select a subset of the original dataset. Then the selected subset will be split into a training and a testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data = pp_action.pre_process(config)\n",
    "pp_action.split_show_data(samples_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uuids = samples_data.index[samples_data['selected'] == 1].tolist()\n",
    "x_train = samples_data.index[samples_data['train'] == 1].tolist()\n",
    "x_dev = samples_data.index[samples_data['dev'] == 1].tolist()\n",
    "x_test = samples_data.index[samples_data['test'] == 1].tolist()\n",
    "y_train = samples_data.fam_num[samples_data['train'] == 1].tolist()\n",
    "y_dev = samples_data.fam_num[samples_data['dev'] == 1].tolist()\n",
    "y_test = samples_data.fam_num[samples_data['test'] == 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Since the DAN required a very considerable amount fo time for the training processes we will try reducing the dimensionality of the dataset.\n",
    "\n",
    "We would also like this approach to be scalable to the entire balanced dataset so we will load sparse representations of the data vectors.\n",
    "\n",
    "To achieve this we will use the Principal Component Analysis in order to operate on the sparse vectros. Let's define two helper functions first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_pca(config, i_pca, samples, load_batch_size):\n",
    "    t = 0\n",
    "    \n",
    "    while t < len(samples):\n",
    "        data = loader_tfidf.load_tfidf(config, samples[t : t + load_batch_size], dense=True, ordered=False)\n",
    "        t += load_batch_size\n",
    "\n",
    "        i_pca.partial_fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_data(config, i_pca, samples, load_batch_size):\n",
    "    new_data = [] \n",
    "    t = 0\n",
    "    \n",
    "    while t < len(samples):\n",
    "        data = loader_tfidf.load_tfidf(config, samples[t : t + load_batch_size], dense=True, ordered=True)\n",
    "        t += load_batch_size\n",
    "\n",
    "        new_data.append(i_pca.transform(data))\n",
    "        \n",
    "    return np.concatenate(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_pca = IncrementalPCA(n_components=1024, batch_size=load_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the PCA algorithm incrementally only on the trainining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pca(config, i_pca, random.sample(x_train, len(x_train)), load_batch_size)\n",
    "joblib.dump(i_pca, 'temp_pca_1000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# or directly load the trained PCA model if available\n",
    "i_pca = joblib.load('temp_pca_1000.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i_pca.explained_variance_ratio_.sum())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will use the trained algorithm to (incrementally) transform all the data vectors. This will allow us to transform larger dataset than what would fit in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transform_data(config, i_pca, x_train, load_batch_size)\n",
    "X_dev = transform_data(config, i_pca, x_dev, load_batch_size)\n",
    "X_test = transform_data(config, i_pca, x_test, load_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = X_train[i] / X_train.shape[1]\n",
    "X_train = X_train.T\n",
    "\n",
    "for i in range(X_dev.shape[0]):\n",
    "    X_dev[i] = X_dev[i] / X_dev.shape[1]\n",
    "X_dev = X_dev.T\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i] = X_test[i] / X_test.shape[1]\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels pre-processing\n",
    "\n",
    "We will initially convert the true labels into a one-hot vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(set(y_train))\n",
    "n_classes = len(classes)\n",
    "\n",
    "classes_dict = dict(zip(classes, range(n_classes)))\n",
    "y_train = [classes_dict[i] for i in y_train]\n",
    "y_dev = [classes_dict[i] for i in y_dev]\n",
    "y_test = [classes_dict[i] for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "Y_train = lb.fit_transform(y_train).T\n",
    "Y_dev = lb.fit_transform(y_dev).T\n",
    "Y_test = lb.fit_transform(y_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_dev shape: \" + str(X_dev.shape))\n",
    "print (\"Y_dev shape: \" + str(Y_dev.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Hyper-parameters\n",
    "\n",
    "Let's set the hyper-paramters, we will try to start with a fast network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "n_epochs = 384\n",
    "minibatch_size = 256\n",
    "n_h_layers = 3\n",
    "ls = [[256,X_train.shape[0]], [256,1], [128,256], [128,1], [Y_train.shape[0],128], [Y_train.shape[0],1]]\n",
    "# ls = [[512,X_train.shape[0]], [512,1], [256, 512], [256,1], [128,256], [128,1], [Y_train.shape[0],128], [Y_train.shape[0],1]]\n",
    "# ls = [[128,X_train.shape[0]], [128,1], [Y_train.shape[0],128], [Y_train.shape[0],1]]\n",
    "# ls = [[512,X_train.shape[0]], [512,1], [Y_train.shape[0],512], [Y_train.shape[0],1]]\n",
    "keep_probs = 0.9\n",
    "reg = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model definition\n",
    "\n",
    "At each step the vectors will go through a softmax function.\n",
    "\n",
    "First let's define some placeholders for the input X and the labels Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_ph(n_feats, n_classes):\n",
    "    with tf.device('/gpu:0'):\n",
    "        X = tf.placeholder(dtype=tf.float32, shape=(n_feats, None))\n",
    "        Y = tf.placeholder(dtype=tf.float32, shape=(n_classes, None))\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        return X,Y, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the wiehgts using the Xavier intialization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(n_layers, layer_sizes):\n",
    "    params = {}\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        for i in range(n_layers):\n",
    "            Wn = 'W{}'.format(i)\n",
    "            bn = 'b{}'.format(i)\n",
    "            \n",
    "            params[Wn] = tf.get_variable(\n",
    "                Wn, \n",
    "                layer_sizes[i * 2], \n",
    "                initializer = tf.contrib.layers.xavier_initializer(seed = 1)\n",
    "            )\n",
    "            \n",
    "            params[bn] = tf.get_variable(\n",
    "                bn, \n",
    "                layer_sizes[(i * 2) + 1],\n",
    "                initializer = tf.zeros_initializer()\n",
    "            )\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fwd(X, params, keep_prob):\n",
    "    Zn = None\n",
    "    epsilon = 1e-4\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "#         An = X\n",
    "        An = tf.nn.dropout(X, keep_prob)\n",
    "        \n",
    "        for i in range(n_h_layers):\n",
    "            Wn = 'W{}'.format(i)\n",
    "            bn = 'b{}'.format(i)\n",
    "            \n",
    "            Zn = tf.add(tf.matmul(params[Wn], An), params[bn])\n",
    "            \n",
    "            batch_mean, batch_var = tf.nn.moments(Zn,[0])\n",
    "            BN = tf.nn.batch_normalization(\n",
    "                x=Zn,\n",
    "                mean=batch_mean,\n",
    "                variance=batch_var,\n",
    "                offset=None,\n",
    "                scale=None,\n",
    "                variance_epsilon=epsilon\n",
    "            )\n",
    "            \n",
    "            An = tf.nn.dropout(tf.nn.relu(BN), keep_prob)\n",
    "            \n",
    "    return Zn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Zn, Y, reg, params, n_layers):\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        logits = tf.transpose(Zn)\n",
    "        labels = tf.transpose(Y)\n",
    "        \n",
    "        regularization = 0.0\n",
    "        for i in range(n_layers):\n",
    "            Wn = 'W{}'.format(i)\n",
    "            regularization += tf.nn.l2_loss(params[Wn])\n",
    "        \n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)) + (reg * regularization)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finally the DAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dan(X_train, Y_train, X_dev, Y_dev, l_rate, n_epochs, minibatch_size, n_h_layers, layers, k_prob, reg):\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        X, Y, keep_prob = init_ph(X_train.shape[0], Y_train.shape[0])\n",
    "\n",
    "        params = init_weights(n_h_layers, layers)\n",
    "        \n",
    "        Z = fwd(X, params, keep_prob)\n",
    "        \n",
    "        cost = compute_cost(Z, Y, reg, params, n_h_layers)\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        learning_rate = tf.train.exponential_decay(l_rate, global_step, 5000, 0.96)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost, global_step=global_step)\n",
    "        \n",
    "        y_pred = tf.argmax(Z)\n",
    "        \n",
    "        y_true = tf.argmax(Y)\n",
    "        \n",
    "        correct_prediction = tf.equal(y_pred, y_true)\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    init = tf.global_variables_initializer()  \n",
    "\n",
    "    sess =  tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "    num_minibatches = int(X_train.shape[1] / minibatch_size)\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_cost = 0.\n",
    "\n",
    "        minibatch_idxs = np.random.permutation(X_train.shape[1])\n",
    "\n",
    "        for i in range(num_minibatches):\n",
    "\n",
    "            minibatch_X = np.take(\n",
    "                X_train,\n",
    "                minibatch_idxs[i * minibatch_size : (i + 1) * minibatch_size], \n",
    "                axis=1\n",
    "            )\n",
    "            minibatch_Y = np.take(\n",
    "                Y_train, \n",
    "                minibatch_idxs[i * minibatch_size : (i + 1) * minibatch_size], \n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            _ , minibatch_cost = sess.run(\n",
    "                [optimizer, cost], \n",
    "                feed_dict={\n",
    "                    X: minibatch_X, \n",
    "                    Y: minibatch_Y,\n",
    "                    keep_prob: k_prob\n",
    "                }\n",
    "            )\n",
    "\n",
    "            epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train, keep_prob: 1.0}, session=sess))\n",
    "            print (\"Dev Accuracy:\", accuracy.eval({X: X_dev, Y: Y_dev, keep_prob: 1.0}, session=sess))\n",
    "            print (\"Learning Rate:\", learning_rate.eval(session=sess))\n",
    "            print (\"\")\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            costs.append(epoch_cost)\n",
    "\n",
    "\n",
    "    tr_acc =  accuracy.eval({X: X_train, Y: Y_train, keep_prob: 1.0}, session=sess)\n",
    "    dv_acc = accuracy.eval({X: X_dev, Y: Y_dev, keep_prob: 1.0}, session=sess)\n",
    "\n",
    "    print (\"Train Accuracy:\",tr_acc)\n",
    "    print (\"Dev Accuracy:\", dv_acc)\n",
    "\n",
    "    return params, costs, tr_acc, dv_acc, accuracy, sess, X, Y, keep_prob, y_pred, y_true \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1)\n",
    "costs = []\n",
    "\n",
    "parameters, cost_list, tr_acc, dv_acc, accuracy, sess, X, Y, keep_prob, y_pred, y_true = dan(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_dev,\n",
    "    Y_dev,\n",
    "    learning_rate,\n",
    "    n_epochs,\n",
    "    minibatch_size,\n",
    "    n_h_layers,\n",
    "    ls,\n",
    "    keep_probs,\n",
    "    reg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the network has been trained, let's see how it behaves on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts_acc, y_predicted, y_labels = sess.run(\n",
    "    [accuracy, y_pred, y_true], \n",
    "    feed_dict={X: X_test, Y: Y_test, keep_prob: 1.0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    x = np.arange(len(costs)),\n",
    "    y = costs\n",
    ")\n",
    "ply.iplot([trace], filename='costs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After having trained our NN let's see how it behaves on the real test set. To get a sense of the performance of the network we will look at the F1 score, which is the harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = f1_score(y_labels, y_predicted, average=None)\n",
    "print(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Train Accuracy:\",tr_acc)\n",
    "print (\"Dev Accuracy:\", dv_acc)\n",
    "print (\"Test Accuracy:\", ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test F1 Average Score:', f1_score(y_labels, y_predicted, average='ma'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a very nice result. Let's see the detail of the score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_fams = samples_data.family[samples_data['test'] == 1].tolist()\n",
    "y_test_fams_num = samples_data.fam_num[samples_data['test'] == 1].tolist()\n",
    "\n",
    "class_fam = {}\n",
    "for i in range(len(y_test_fams)):\n",
    "    class_fam[classes_dict[y_test_fams_num[i]]] = y_test_fams[i]\n",
    "\n",
    "fam_score = {}\n",
    "for fam_num, fam in class_fam.items():\n",
    "    fam_score[fam] = f1s[fam_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fam, score in sorted(fam_score.items()):\n",
    "    print('{:20} {:20}'.format(fam, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's look at the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_labels, y_predicted).astype(float)\n",
    "for vec in cm:\n",
    "    vec /= np.sum(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "families = [class_fam[i] for i in sorted(class_fam.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Heatmap(z=cm, x=families, y=families)\n",
    "ply.iplot([trace], filename='conf_matrix_28k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
